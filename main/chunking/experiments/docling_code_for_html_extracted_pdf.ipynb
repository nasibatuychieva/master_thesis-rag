{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2095bcb7-7e6c-4ea1-a7b6-61f2ec62df9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install docling\n",
    "!pip install -qU pip docling transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf0ef3ac-aa06-4732-ae8d-dd7445d06678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.dbutils import DBUtils\n",
    "import re, os\n",
    "\n",
    "def get_repo_root() -> Path:\n",
    "    dbutils = DBUtils(spark)\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "    # Extrahiere /Workspace/Repos/<user>/<repo>\n",
    "    match = re.match(r\"^(\\/Workspace\\/Repos\\/[^\\/]+\\/[^\\/]+)\", notebook_path)\n",
    "    if match:\n",
    "        return Path(match.group(1))\n",
    "    else:\n",
    "        return Path(os.getcwd())\n",
    "\n",
    "repo_root = get_repo_root()\n",
    "print(\"üìÅ Repo root:\", repo_root)\n",
    "\n",
    "# Eine Ebene h√∂her (wie \"cd ..\")\n",
    "parent_path = repo_root.parent\n",
    "\n",
    "# Zwei Ebenen h√∂her\n",
    "two_up = repo_root.parent.parent\n",
    "\n",
    "print(\"‚¨ÜÔ∏è Eine Ebene h√∂her:\", parent_path)\n",
    "print(\"‚¨ÜÔ∏è root:\", parent_path)\n",
    "\n",
    "doc_root = parent_path / \"documents\"\n",
    "out_dir = parent_path / \"out\"\n",
    "product_name = \"Nano 33 BLE\"\n",
    "product_category = \"Nano Family\"\n",
    "file_name  = \"Nano_33_BLE_datasheet.pdf\"\n",
    "\n",
    "pdf_path = doc_root / product_category / product_name / \"Nano_33_BLE_datasheet.pdf\"\n",
    "print(\"‚¨ÜÔ∏è pdf pfad:\",pdf_path)\n",
    "\n",
    "out_path_docling = out_dir / product_category / product_name / \"docling_chunks.jsonl\"\n",
    "print(\"‚¨ÜÔ∏è oout_path_docling:\",out_path_docling)\n",
    "\n",
    "out_path_langchain = out_dir / product_category / product_name / \"langchain_chunks.jsonl\"\n",
    "print(\"‚¨ÜÔ∏è out_path_langchain:\",out_path_langchain)\n",
    "\n",
    "code_path_langchain = parent_path / \"langchain_chunking.py\"\n",
    "print(\"‚¨ÜÔ∏ècode_path_langchain:\",code_path_langchain)\n",
    "\n",
    "code_path_docling= parent_path / \"docling_chunking.py\"\n",
    "print(\"‚¨ÜÔ∏ècode_path_langchain:\",code_path_docling)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13a9d61-7e75-4375-b79f-36acb1456afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from pyspark.dbutils import DBUtils\n",
    "import os\n",
    "\n",
    "def get_repo_root() -> Path:\n",
    "    dbutils = DBUtils(spark)\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "    match = re.match(r\"^(\\/Workspace\\/Users\\/[^\\/]+\\/[^\\/]+)\", notebook_path)\n",
    "    if match:\n",
    "        return Path(match.group(1))\n",
    "    else:\n",
    "        return Path(os.getcwd())\n",
    "\n",
    "repo_root = get_repo_root()\n",
    "print(\"üìÅ Repo root:\", repo_root)\n",
    "\n",
    "parent_path = repo_root.parent\n",
    "doc_root = parent_path / \"documents\"\n",
    "out_dir = parent_path / \"out\"\n",
    "\n",
    "# ‚úÖ Liste der zu verarbeitenden Unterordner\n",
    "allowed_folders = [\n",
    "    \"Arduino Cloud\",\n",
    "    \"Arduino CLI\",\n",
    "]\n",
    "\n",
    "def iterate_product_docs(doc_root: Path, out_dir: Path):\n",
    "    \"\"\"Durchl√§uft nur die erlaubten Unterordner von documents/\"\"\"\n",
    "    for folder in allowed_folders:\n",
    "        folder_path = doc_root / folder\n",
    "        if not folder_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  Ordner nicht gefunden: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        # jetzt PDFs innerhalb dieses Ordners suchen\n",
    "        for pdf_path in folder_path.rglob(\"*.pdf\"):\n",
    "            if len(pdf_path.parts) >= 3:\n",
    "                print(f\"üìÇ {pdf_path.parent.parent.name} / {pdf_path.parent.name}\")\n",
    "                #process_pdf(pdf_path, out_dir)\n",
    "\n",
    "# üöÄ Aufruf\n",
    "iterate_product_docs(doc_root, out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accd71fd-5f47-4be0-ac57-e610cf07e1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_path = \"/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/documents/Arduino Cloud/Cloud Editor/Cloud Editor.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4080490-68e9-4231-9d1d-32d077a35fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "pdf_options = PdfPipelineOptions()\n",
    "pdf_options.do_ocr = False                    # No OCR - pure text extraction only   # noqa: E501\n",
    "pdf_options.generate_page_images = False      # No page images  # noqa: E501\n",
    "pdf_options.generate_picture_images = False   # Ignore pictures completely  # noqa: E501\n",
    "pdf_options.generate_table_images = False     # Keep tables as text/markdown, not images  # noqa: E501\n",
    "\n",
    "        # Configure format options\n",
    "format_options = {\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pdf_options\n",
    "            )\n",
    "        }\n",
    "\n",
    "# Initialize document converter\n",
    "converter_2 = DocumentConverter(\n",
    "            format_options=format_options\n",
    "        )\n",
    "result_2 = converter_2.convert(pdf_path)\n",
    "doc_2 = result_2.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3e521b-8a48-46af-b76a-d153bb501158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    TesseractCliOcrOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "ocr_options = TesseractCliOcrOptions(lang=[\"auto\"])\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "        do_ocr=True, force_full_page_ocr=True, ocr_options=ocr_options\n",
    "    )\n",
    "\n",
    "converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pipeline_options,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "doc = converter.convert(pdf_path).document\n",
    "md = doc.export_to_markdown()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9697b017-054d-4962-a90b-121359c81cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    TesseractCliOcrOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "\n",
    "ocr_options = TesseractCliOcrOptions(lang=[\"auto\"])\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "        do_ocr=True, force_full_page_ocr=True, ocr_options=ocr_options\n",
    "    )\n",
    "\n",
    "converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pipeline_options,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "doc = converter.convert(pdf_path).document\n",
    "md = doc.export_to_markdown()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2b23d5-6cfb-427f-8480-b3e080b4b3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f564afbf-03f4-4059-b932-6e5853e8a7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def ocr_pdf_with_tesseract(pdf_path: str, dpi: int = 300, lang: str = \"eng+deu\") -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    out = []\n",
    "    for page in doc:\n",
    "        # 1) Normale Textschicht versuchen\n",
    "        txt = page.get_text(\"text\").strip()\n",
    "        if txt:\n",
    "            out.append(txt)\n",
    "            continue\n",
    "        # 2) Sonst OCR auf gerenderter Seite\n",
    "        pix = page.get_pixmap(dpi=dpi)\n",
    "        mode = \"RGB\" if pix.n < 4 else \"RGBA\"\n",
    "        img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)\n",
    "        out.append(pytesseract.image_to_string(img, lang=lang))\n",
    "    doc.close()\n",
    "    return \"\\n\\n\".join(out).strip()\n",
    "\n",
    "raw_text = ocr_pdf_with_tesseract(pdf_path, dpi=300, lang=\"eng+deu\")\n",
    "print(raw_text[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c39cde7-ac4a-4e4c-83a8-f1cce4a50b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= 1) Hilfsfunktionen =========\n",
    "def is_heading(line: str) -> bool:\n",
    "    s = line.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    # kurz + kaum Satzzeichen\n",
    "    if len(s) <= 80 and sum(ch in \".:;|/[]()\" for ch in s) <= 1:\n",
    "        # ALL CAPS oder Title Case oder endet nicht mit Punkt\n",
    "        if (s.isupper() and any(c.isalpha() for c in s)) or \\\n",
    "           (s.istitle() and \" \" in s) or \\\n",
    "           not s.endswith(\".\"):\n",
    "            # wenig Ziffern\n",
    "            digits = sum(ch.isdigit() for ch in s)\n",
    "            if digits <= max(2, len(s)//10):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def split_into_sections(cleaned_text: str):\n",
    "    sections = []\n",
    "    cur_title = \"\"\n",
    "    cur_buf = []\n",
    "    for line in cleaned_text.splitlines():\n",
    "        if is_heading(line):\n",
    "            # flush\n",
    "            if cur_buf:\n",
    "                sections.append((cur_title, \"\\n\".join(cur_buf).strip()))\n",
    "                cur_buf = []\n",
    "            cur_title = line.strip()\n",
    "        else:\n",
    "            cur_buf.append(line)\n",
    "    if cur_buf:\n",
    "        sections.append((cur_title, \"\\n\".join(cur_buf).strip()))\n",
    "    # Notfall: wenn keine Headings erkannt ‚Üí alles in eine Section\n",
    "    if not sections:\n",
    "        sections = [(\"\", cleaned_text)]\n",
    "    return sections\n",
    "\n",
    "def approx_token_count(text: str) -> int:\n",
    "    # sehr grobe Approximation: ~4 Zeichen pro Token\n",
    "    return max(1, int(len(text) / 4))\n",
    "\n",
    "def semantic_density_heuristic(text: str) -> float:\n",
    "    # einfache, stabile Heuristik: Anteil einzigartiger W√∂rter * Bonus f√ºr Satzl√§nge\n",
    "    words = re.findall(r\"[A-Za-z√Ä-√ø0-9]+\", text.lower())\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    uniq = len(set(words))\n",
    "    ratio = uniq / len(words)                      # 0..1\n",
    "    # l√§ngere S√§tze deuten oft auf inhaltliche Dichte\n",
    "    sentences = re.split(r\"[.!?]\\s+\", text.strip())\n",
    "    avg_len = sum(len(s.split()) for s in sentences if s) / max(1, len(sentences))\n",
    "    bonus = min(1.0, avg_len / 25.0)               # capped\n",
    "    score = 0.6 * ratio + 0.4 * bonus\n",
    "    return round(min(1.0, max(0.0, score)), 3)\n",
    "\n",
    "def chunk_section(text: str, target_tokens=800, overlap_tokens=120):\n",
    "    # wir chunk‚Äôen auf Token-Approximation; schneiden an Wortgrenzen\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        # Zielspanne in W√∂rtern sch√§tzen (1 Token ~ 0.75 Wort bei dieser Approx)\n",
    "        target_words = int(target_tokens * 0.75)\n",
    "        end = min(len(words), start + target_words)\n",
    "        chunk = \" \".join(words[start:end]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if end == len(words):\n",
    "            break\n",
    "        # Overlap\n",
    "        overlap_words = int(overlap_tokens * 0.75)\n",
    "        start = max(start + target_words - overlap_words, start + 1)\n",
    "    return chunks\n",
    "\n",
    "# ========= 2) Hauptroutine: OCR-Text ‚Üí Clean ‚Üí Sections ‚Üí Chunks ‚Üí JSONL =========\n",
    "def chunk_ocr_text_to_jsonl(\n",
    "    raw_text: str,\n",
    "    category: str,\n",
    "    product: str,\n",
    "    out_dir: str,\n",
    "    base_id: str = \"ocr\",\n",
    "    target_tokens: int = 800,\n",
    "    overlap_tokens: int = 120\n",
    "):\n",
    "    # 2.1 Cleaning ‚Äì nimm hier DEINE erweiterte Funktion\n",
    "    def clean_html_extracted_pdf_text(t: str) -> str:\n",
    "        # >>>> ERSETZE bei Bedarf durch deine Version mit Ligaturen etc. <<<<\n",
    "        t = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", t)\n",
    "        t = t.replace(\"\\r\", \"\")\n",
    "        t = re.sub(r\"[ \\t]{2,}\", \" \", t)\n",
    "        # Navigations-/M√ºllzeilen optional herausfiltern\n",
    "        t = re.sub(r\"^\\s*(Go Back|ON THIS PAGE|Author.*|Last revision.*|Help|Arduino\\s*Docs)\\s*$\",\n",
    "                   \"\", t, flags=re.I|re.M)\n",
    "        t = re.sub(r\"\\n{2,}\", \"\\n\", t)\n",
    "        return t.strip()\n",
    "\n",
    "    cleaned = clean_html_extracted_pdf_text(raw_text)\n",
    "\n",
    "    # 2.2 Sections erkennen\n",
    "    sections = split_into_sections(cleaned)\n",
    "\n",
    "    # 2.3 pro Section chunken\n",
    "    records = []\n",
    "    total_chunks = 0\n",
    "    for s_idx, (section_title, section_text) in enumerate(sections, start=1):\n",
    "        if not section_text.strip():\n",
    "            continue\n",
    "        raw_chunks = chunk_section(section_text, target_tokens, overlap_tokens)\n",
    "        for c_idx, ch in enumerate(raw_chunks, start=1):\n",
    "            total_chunks += 1\n",
    "            chunk_id = f\"{category}::{product}::{base_id}::s{s_idx}::c{c_idx}\"\n",
    "            rec = {\n",
    "                \"category\": category,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk_size\": int(len(ch)),                # oder approx_token_count(ch)\n",
    "                \"chunk_type\": \"semantic\",                  # frei setzbar: \"semantic\"\n",
    "                \"product\": product,\n",
    "                \"section\": section_title or \"\",\n",
    "                \"semantic_density\": float(semantic_density_heuristic(ch)),\n",
    "                \"text\": ch,\n",
    "                \"total_chunks\": 0,                         # f√ºllen wir nachher\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "    # 2.4 total_chunks nachtragen\n",
    "    for rec in records:\n",
    "        rec[\"total_chunks\"] = total_chunks\n",
    "\n",
    "    # 2.5 JSONL speichern\n",
    "    out_path = Path(out_dir) / category / product\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_path / \"chunks_sample.jsonl\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return str(out_file), total_chunks\n",
    "\n",
    "# ======= Beispiel-Aufruf =======\n",
    "# raw_text = ...   # kommt von deiner PaddleOCR-Pipeline\n",
    "category = \"Arduino Cloud\"\n",
    "product  = \"Cloud Editor\"\n",
    "out_dir  = \"main/out\"\n",
    "out_file, n = chunk_ocr_text_to_jsonl(raw_text, category, product, out_dir)\n",
    "print(\"Geschrieben:\", out_file, \" | Chunks:\", n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6ec13a-1eb8-45fd-a1cd-0f3f1abae8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/chunking/main/out/Arduino Cloud/Cloud Editor/chunks_sample.jsonl\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae17b6f-514d-46cf-9535-458955456165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f719b4d4-8218-4907-ae40-09faf59a9bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install betterhtmlchunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a107d70-ee60-4332-9609-f143143132fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from betterhtmlchunking import DomRepresentation\n",
    "from betterhtmlchunking.main import ReprLengthComparisionBy\n",
    "from betterhtmlchunking.main import tag_list_to_filter_out\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"content\">\n",
    "      <h1>Document Title</h1>\n",
    "      <p>First paragraph...</p>\n",
    "      <p>Second paragraph...</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create document representation with 20 character chunks.\n",
    "dom_repr = DomRepresentation(\n",
    "    MAX_NODE_REPR_LENGTH=20,\n",
    "    website_code=html_content,\n",
    "    repr_length_compared_by=ReprLengthComparisionBy.HTML_LENGTH\n",
    "    # tag_list_to_filter_out=[\"/head\", \"/header\", \"...\"]  # By default tag_list_to_filter_out is used.\n",
    ")\n",
    "dom_repr.start()\n",
    "\n",
    "# Render HTML:\n",
    "for idx in dom_repr.tree_regions_system.sorted_roi_by_pos_xpath:\n",
    "    print(\"*\" * 50)\n",
    "    print(f\"IDX: {idx}\")\n",
    "    roi_html_render: str =\\\n",
    "        dom_repr.render_system.get_roi_html_render_with_pos_xpath(\n",
    "            roi_idx=idx\n",
    "        )\n",
    "    print(roi_html_render)\n",
    "\n",
    "\n",
    "# Render text:\n",
    "for idx in dom_repr.tree_regions_system.sorted_roi_by_pos_xpath:\n",
    "    print(\"*\" * 50)\n",
    "    print(f\"IDX: {idx}\")\n",
    "    roi_text_render: str =\\\n",
    "        dom_repr.render_system.get_roi_text_render_with_pos_xpath(\n",
    "            roi_idx=idx\n",
    "        )\n",
    "    print(roi_text_render)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7c35a2-6b4d-4738-afd2-2283e41c7b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- kleine Helfer ----------\n",
    "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+', re.I)\n",
    "TAG_RE   = re.compile(r'<[^>]+>')                    # √ºbrig gebliebene HTML-Tags\n",
    "SPACES   = re.compile(r'[ \\t]{2,}')\n",
    "MULTI_NL = re.compile(r'\\n{3,}')\n",
    "PAGE_NUM_LINE = re.compile(r'^\\s*\\d+\\s*/\\s*\\d+\\s*$') # \"12 / 26\" etc.\n",
    "\n",
    "def _strip_urls_keep_anchor(text: str) -> str:\n",
    "    # [label](url)  -> label\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\((?:https?://|www\\.)[^\\)]+\\)', r'\\1', text)\n",
    "    # \"label (https://...)\" -> \"label\"\n",
    "    text = re.sub(r'\\s*\\((?:https?://|www\\.)[^\\)]+\\)', '', text)\n",
    "    # nackte URLs l√∂schen\n",
    "    text = URL_RE.sub('', text)\n",
    "    return text\n",
    "\n",
    "def _join_wrapped_lines(s: str) -> str:\n",
    "    # join: Zeilen, die k√ºnstlich umbrechen (HTML->PDF)\n",
    "    out = []\n",
    "    prev = ''\n",
    "    for ln in s.split('\\n'):\n",
    "        l = ln.strip()\n",
    "        if not prev:\n",
    "            prev = l\n",
    "            continue\n",
    "        # join, wenn vorige Zeile nicht mit Satzende endet UND diese klein beginnt\n",
    "        if prev and l and not re.search(r'[.;:!?‚Ä¶]$', prev) and l[:1].islower():\n",
    "            prev = prev + ' ' + l\n",
    "        else:\n",
    "            out.append(prev)\n",
    "            prev = l\n",
    "    if prev:\n",
    "        out.append(prev)\n",
    "    return '\\n'.join(out)\n",
    "\n",
    "def _drop_frequent_header_footer(text: str, freq_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Entfernt Zeilen, die auf sehr vielen Seiten wiederkehren (Header/Footer).\n",
    "    Erwartet, dass Seitenumbr√ºche als '\\f' oder viele Leerzeilen vorkommen.\n",
    "    Fallback: Wir betrachten alle Zeilen global.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in text.split('\\n') if ln.strip()]\n",
    "    if not lines:\n",
    "        return text\n",
    "    counts = Counter(lines)\n",
    "    n = len(lines)\n",
    "    to_drop = {ln for ln, c in counts.items() if c / n > freq_threshold}\n",
    "\n",
    "    cleaned = []\n",
    "    for ln in text.split('\\n'):\n",
    "        s = ln.strip()\n",
    "        if s in to_drop:\n",
    "            continue\n",
    "        if PAGE_NUM_LINE.match(s):\n",
    "            continue\n",
    "        cleaned.append(ln)\n",
    "    return '\\n'.join(cleaned)\n",
    "\n",
    "def normalize_heading(title: str) -> str:\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    s = title.strip().lower()\n",
    "    s = re.sub(r'^\\s*(section|chapter|kapitel|abschnitt)\\s*\\d+[:.)-]*\\s*', '', s, flags=re.I)\n",
    "    s = re.sub(r'^\\s*\\d+(?:\\.\\d+)*\\s*[:.)-]*\\s*', '', s)\n",
    "    s = s.rstrip(' :.-')\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "HEADINGS_BLACKLIST_EQ = {\n",
    "    \"contents\",\"table of contents\",\"toc\",\"index\",\"references\",\"reference documentation\",\n",
    "    \"company information\",\"company info\",\"revision history\",\"document history\",\"legal notice\",\n",
    "    \"trademarks\",\"acknowledgements\",\"glossary\",\"contacts\",\"contact\",\n",
    "    \"inhalt\",\"inhaltsverzeichnis\",\"verzeichnis\",\"referenzen\",\"referenzdokumentation\",\n",
    "    \"unternehmensinformationen\",\"revision\",\"versionsverlauf\",\"rechtliche hinweise\",\n",
    "    \"marken\",\"danksagungen\",\"glossar\",\"kontakt\",\n",
    "}\n",
    "HEADINGS_BLACKLIST_CONTAINS = {\n",
    "    \"reference documentation\",\"referenzdokumentation\",\"table of contents\",\"inhaltsverzeichnis\",\n",
    "    \"revision history\",\"document history\",\"company information\",\"on this page\"\n",
    "}\n",
    "\n",
    "def title_matches_blacklist(title: str) -> bool:\n",
    "    tnorm = normalize_heading(title)\n",
    "    return (tnorm in HEADINGS_BLACKLIST_EQ) or any(k in tnorm for k in HEADINGS_BLACKLIST_CONTAINS)\n",
    "\n",
    "def _drop_sidebar_blocks(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Entfernt typische Sidebar-/TOC-Bl√∂cke aus HTML-Seiten-PDFs, z.B.:\n",
    "    - Breadcrumbs \"Home / ... \"\n",
    "    - \"ON THIS PAGE\" Liste\n",
    "    - rechte Navigationsspalte mit vielen Stichpunkten\n",
    "    \"\"\"\n",
    "    # Breadcrumb-Zeilen\n",
    "    text = re.sub(r'^\\s*Home\\s*/.*$', '', text, flags=re.I|re.M)\n",
    "\n",
    "    # \"ON THIS PAGE\" bis zur n√§chsten Leerzeilen-L√ºcke oder Abschnitts√ºberschrift\n",
    "    text = re.sub(\n",
    "        r'(?ims)^\\s*(on\\s+this\\s+page)\\s*\\n(?:.+\\n){1,30}?(?=\\n\\s*\\n|^[A-Z].{0,80}$|^#|\\Z)',\n",
    "        '',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # Link-Listen/Tabellen (viele Pipes oder viele \"‚Ä¢\" in kurzer Folge)\n",
    "    lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0382c9a5-683b-4ab4-b5f6-657d74ad66e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf158ded-e7e3-4fe6-abec-cfdc746edef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_langchain = out_dir / product_category / product_name / \"langchain_chunks.jsonl\"\n",
    "print(\"‚¨ÜÔ∏è out_path_langchain:\",out_path_langchain)\n",
    "\n",
    "code_path_langchain = parent_path / \"langchain_chunking.py\"\n",
    "print(\"‚¨ÜÔ∏ècode_path_langchain:\",code_path_langchain)\n",
    "\n",
    "code_path_docling= parent_path / \"docling_chunking.py\"\n",
    "print(\"‚¨ÜÔ∏ècode_path_langchain:\",code_path_docling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d772da9-619a-4ab9-8715-8592fb349b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def process_pdf(pdf_path: Path, out_dir: Path):\n",
    "    category = pdf_path.parent.parent.name\n",
    "    product  = pdf_path.parent.name\n",
    "\n",
    "    out_path = out_dir / category / \"docling_chunks.jsonl\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #doc = converter.convert(str(pdf_path)).document\n",
    "    raw_chunks = list(chunker.chunk(dl_doc=doc_2))\n",
    "    total_chunks = len(raw_chunks)\n",
    "\n",
    "    records = []\n",
    "    for i, ch in enumerate(raw_chunks):\n",
    "        text_raw = clean_text(ch.text or \"\")\n",
    "        if len(text_raw) < 30:\n",
    "            continue\n",
    "\n",
    "        context = clean_text(chunker.contextualize(chunk=ch))\n",
    "        if len(context.split()) < 25:\n",
    "            continue\n",
    "\n",
    "        if should_drop_chunk(ch, context):  # optional\n",
    "            continue\n",
    "\n",
    "        # section (defensiv)\n",
    "        section = None\n",
    "        hp = getattr(ch, \"hierarchy_path\", None)\n",
    "        if isinstance(hp, list) and hp:\n",
    "            last = hp[-1]\n",
    "            if isinstance(last, dict):\n",
    "                section = last.get(\"title\")\n",
    "\n",
    "        n_tokens = tokenizer.count_tokens(context)\n",
    "        semantic_density = round(n_tokens / max(1, len(context)), 4)\n",
    "\n",
    "        rec = {\n",
    "            \"category\": category,\n",
    "            \"chunk_id\": f\"{pdf_path.stem}::c{i}\",\n",
    "            \"chunk_size\": n_tokens,\n",
    "            \"chunk_type\": \"contextualized\",\n",
    "            \"product\": product,\n",
    "            \"section\": section,\n",
    "            \"semantic_density\": semantic_density,\n",
    "            \"text\": f\"[Product: {product}] [Category: {category}]\\n\\n{context}\",\n",
    "            \"total_chunks\": total_chunks,\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[OK] {len(records)} Chunks hinzugef√ºgt zu: {out_path}\")\n",
    "\n",
    "def iterate_product_docs(doc_root: Path, out_dir: Path):\n",
    "    for pdf_path in doc_root.rglob(\"*.pdf\"):\n",
    "        # erwartet Struktur <root>/<category>/<product>/<file.pdf>\n",
    "        if len(pdf_path.parts) >= 3:\n",
    "            print(f\"üìÇ {pdf_path.parent.parent.name} / {pdf_path.parent.name}\")\n",
    "            process_pdf(pdf_path, out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2cc8aeb-b7c1-4653-b74f-f392e604dac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "doc_root = parent_path / \"documents\"\n",
    "out_dir = parent_path / \"out\"\n",
    "# product_name = \"Nano 33 BLE\"\n",
    "# product_category = \"Nano Family\"\n",
    "# file_name  = \"Nano_33_BLE_datasheet.pdf\"\n",
    "\n",
    "print(\"‚¨ÜÔ∏è doc_root:\",doc_root)\n",
    "\n",
    "print(\"‚¨ÜÔ∏è out_dir:\",out_dir)\n",
    "\n",
    "pdf_path = doc_root / product_category / product_name / \"Nano_33_BLE_datasheet.pdf\"\n",
    "print(\"‚¨ÜÔ∏è pdf pfad:\",pdf_path)\n",
    "\n",
    "out_path_docling = out_dir / product_category / product_name / \"docling_chunks.jsonl\"\n",
    "print(\"‚¨ÜÔ∏è oout_path_docling:\",out_path_docling)\n",
    "\n",
    "iterate_product_docs(doc_root, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49adffaf-dd4e-4414-b4ac-666f42d45a2a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"chunk_id\":170},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761497709122}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/out/Nano Family/docling_chunks.jsonl\"\n",
    "df = spark.read.json(out_path_docling)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd855189-7848-45c0-b8c0-f20ddd4861ca",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761498728198}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/out/UNO Family/docling_chunks.jsonl\"\n",
    "df = spark.read.json(out_path_docling)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1366f9f0-4940-4135-b959-68c7ac91afa7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761504242638}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/out/Education/docling_chunks.jsonl\"\n",
    "df = spark.read.json(out_path_docling)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20e3849-977f-43d8-a609-7060ae3faf48",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761591662283}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/out/MKR Family/docling_chunks.jsonl\"\n",
    "df = spark.read.json(out_path_docling)\n",
    "display(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbab7fb-e466-4b05-b073-9ba785427038",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761592099110}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"product\")==\"MKR WAN 1310\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9946e2da-3be5-4dc0-b4a8-b01f6242e3be",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761591666733}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/out/MKR Family/docling_chunks.jsonl\"\n",
    "df_2 = spark.read.json(out_path_docling)\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af7fb47-ca3d-40b6-8162-4f6d8ca64998",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761592207307}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.filter(col(\"product\")==\"MKR WAN 1310\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a036c78a-ae04-492a-83ee-d795f88b42be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(df_2.filter((col(\"text\").contains(\"Arduino Cloud\")) & (col(\"product\") == \"MKR WAN 1310\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4657d646-bf77-4073-8935-fbf1246f5919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(df.filter((col(\"text\").contains(\"Arduino Cloud\")) & (col(\"product\") == \"MKR WAN 1310\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df39f276-384b-45ca-bf4e-6ead2a61f208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/out/Portenta Family/docling_chunks.jsonl\"\n",
    "df = spark.read.json(out_path_docling)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f218b4-7ff5-4c72-86d3-2141fa47acfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/(Clone) out/Portenta Family/docling_chunks.jsonl\"\n",
    "df_2 = spark.read.json(out_path_docling)\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176aeb98-5005-4e0d-9cff-854d353c22cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_2.filter(col(\"product\")==\"Portenta H7\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77fe095-5e08-4662-923a-b86b3d0b2dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(col(\"product\")==\"Portenta H7\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05465f51-81c9-42c7-8c25-40c320b16607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(df.filter((col(\"text\").contains(\"Application Examples\")) & (col(\"product\") == \"Portenta Vision Shield\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ded0744-0c56-4777-bcb3-13468267f541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(df_2.filter((col(\"text\").contains(\"Application Examples\")) & (col(\"product\") == \"Portenta Vision Shield\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3632a70e-d341-4dd7-80e8-05ae2aa8d3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(df.filter((col(\"text\").contains(\"Application Examples\")) )).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea3df4d-83a7-42c1-93ac-afb7d02e5a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "out_path_docling = \"file:/Workspace/Users/nasiba.tuychieva@gea.com/master_thesis-rag/main/out/Arduino Cloud/docling_chunks.jsonl\"\n",
    "df_3 = spark.read.json(out_path_docling)\n",
    "display(df_3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "docling_code_for_html_extracted_pdf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
