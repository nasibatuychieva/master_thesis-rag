{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1124d5c6-3285-479d-8347-1b05cec3b415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import os, json, sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List\n",
    "def resolve_root() -> Path:\n",
    "    candidates: List[Path] = []\n",
    "    if os.environ.get(\"MAIN_ROOT\"):\n",
    "        candidates.append(Path(os.environ[\"MAIN_ROOT\"]))\n",
    "    if getattr(sys, \"argv\", None) and sys.argv and sys.argv[0]:\n",
    "        candidates.append(Path(sys.argv[0]).resolve().parent)\n",
    "    candidates.append(Path(os.getcwd()))\n",
    "    for c in candidates:\n",
    "        for base in [c, *c.parents]:\n",
    "            m = base / \"main\"\n",
    "            if (m / \"documents\").exists():\n",
    "                return m\n",
    "    return Path(os.getcwd()) / \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7b88ee-a97b-4c4e-9dd2-4bcd96076924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "root = resolve_root()\n",
    "doc_root = root / \"documents\"\n",
    "out_file = root / \"out\" / \"chunks_unified.jsonl\"\n",
    "print(root)\n",
    "print(doc_root)\n",
    "print(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78327a51-04fd-45f4-bfe6-54c846fdd5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/chunks_unified.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae39192e-4dbe-4c52-bee0-6bb4cf8ec1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.json(\n",
    "    \"file:/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/Nano Family/Nano 33 BLE/chunks.jsonl\"\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111af669-8f2a-4c9f-8947-ae62d6b90272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2 = spark.read.json(\n",
    "    \"file:/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/Nano Family/Nano 33 BLE/chunks_sample.jsonl\"\n",
    ")\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9e7881-6daf-4bfb-a017-a5b09dda1462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ROOT = get_project_root()\n",
    "\n",
    "DOC_ROOT = ROOT / \"documents\"\n",
    "OUT_DIR  = ROOT / \"out\"\n",
    "OUT_FILE = OUT_DIR / \"chunks_level0.jsonl\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ROOT     = {ROOT}\")\n",
    "print(f\"DOC_ROOT = {DOC_ROOT}\")\n",
    "print(f\"OUT_DIR  = {OUT_DIR}\")\n",
    "print(f\"OUT_FILE = {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "890e28b4-eb58-43ef-968d-eb15fd287a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/chunking/docling/chunker_docling_hybrid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202a3b2c-7fa6-4118-9cf5-08a7f2c290ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root = Path(os.getcwd())       # kein __file__, kein Ärger\n",
    "doc_root = root / \"documents\"\n",
    "out_dir = root / \"out\"\n",
    "\n",
    "print(root)\n",
    "print(doc_root)\n",
    "print(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0820b0be-8628-4646-9363-3d5b5576b51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install docling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e58693-96f5-4b46-b9e0-0e74cba2f1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\n",
    "    \"file:/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents/Nano Family/Nano 33 BLE/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb49b7d-bb58-4acd-b543-cd843d260f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ✅ Neuer offizieller Weg\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Pfad zur Datei (PDF oder HTML)\n",
    "pdf_path_ = \"/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents/Nano Family/Nano 33 BLE/Nano_33_BLE_datasheet.pdf\"\n",
    "# 1️⃣ Converter erzeugen\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# 2️⃣ Datei konvertieren (Docling erkennt Typ automatisch)\n",
    "result = converter.convert(pdf_path_)\n",
    "\n",
    "# 3️⃣ Zugriff auf das Docling-Dokument\n",
    "doc = result.document\n",
    "\n",
    "# 4️⃣ Ausgabe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4080490-68e9-4231-9d1d-32d077a35fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "pdf_options = PdfPipelineOptions()\n",
    "pdf_options.do_ocr = False                    # No OCR - pure text extraction only   # noqa: E501\n",
    "pdf_options.generate_page_images = False      # No page images  # noqa: E501\n",
    "pdf_options.generate_picture_images = False   # Ignore pictures completely  # noqa: E501\n",
    "pdf_options.generate_table_images = False     # Keep tables as text/markdown, not images  # noqa: E501\n",
    "\n",
    "        # Configure format options\n",
    "format_options = {\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pdf_options\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Initialize document converter\n",
    "converter_2 = DocumentConverter(\n",
    "            format_options=format_options\n",
    "        )\n",
    "result_2 = converter_2.convert(pdf_path_)\n",
    "doc_2 = result_2.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d985edd-e2de-4c10-8af7-41c538254164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(doc_2.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f870382f-780a-4254-bd1f-fac0004c8cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(doc.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df4543a-e6d5-464a-9026-ff8d3ee1e80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(doc_2.export_to_markdown()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63a37f2-9a02-404f-9b65-7555e941ee5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU pip docling transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff30d7d-5694-4533-a16d-c41500232129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DOC_SOURCE = doc\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "chunker = HybridChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9991b187-c827-45fa-a848-7f35fcc97d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f\"=== {i} ===\")\n",
    "    print(f\"chunk.text:\\n{f'{chunk.text[:300]}…'!r}\")\n",
    "\n",
    "    enriched_text = chunker.contextualize(chunk=chunk)\n",
    "    print(f\"chunker.contextualize(chunk):\\n{f'{enriched_text[:300]}…'!r}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c39cde7-ac4a-4e4c-83a8-f1cce4a50b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 500  # set to a small number for illustrative purposes\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "    max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    ")\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    merge_peers=True,  # optional, defaults to True\n",
    ")\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "chunks = list(chunk_iter)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c28a2a-b10b-4d68-b9ec-bd0ba4561b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c4501b-dd8b-4c16-90dc-41315d6b17f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Zeilenumbrüche & Silbentrennung\n",
    "    #    „micro-\\ncontroller“ -> „microcontroller“\n",
    "    t = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", t)\n",
    "    t = t.replace(\"\\r\", \"\")\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\", t)             # Mehrfach-Blankzeilen -> eine\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t)           # Mehrfach-Spaces -> einer\n",
    "\n",
    "    # 2) Kopf-/Fußzeilen & Seitenzahlen (heuristisch)\n",
    "    #    Zeilen, die fast nur aus Ziffern/Sonderzeichen bestehen, entfernen\n",
    "    def noisy(line: str) -> bool:\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            return True\n",
    "        non_alpha = sum(1 for ch in s if not ch.isalpha())\n",
    "        return (non_alpha / max(1, len(s))) > 0.6\n",
    "\n",
    "    lines = [ln for ln in t.split(\"\\n\") if not noisy(ln)]\n",
    "    t = \"\\n\".join(lines)\n",
    "\n",
    "    # 3) Tabellen-/Figure-Captions & Tabellenraster raus\n",
    "    t = re.sub(r\"^(Table|Figure)\\s*\\d+[:.\\-]\\s.*$\", \"\", t, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    t = re.sub(r\"^\\s*\\|.*\\|\\s*$\", \"\", t, flags=re.MULTILINE)  # Markdown-Tabellenreihen\n",
    "    t = re.sub(r\"^\\s*[-=]{3,}\\s*$\", \"\", t, flags=re.MULTILINE)\n",
    "\n",
    "    # 4) Kurze Reste weg\n",
    "    t = t.strip()\n",
    "    return t\n",
    "\n",
    "# Schritt 1: Chunking\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "chunks = list(chunk_iter)   # jetzt hast du eine Liste von Chunk-Objekten\n",
    "\n",
    "# Schritt 2: Iteration über rohe Chunks\n",
    "for i, ch in enumerate(chunks):\n",
    "    print(f\"=== Chunk {i} ===\")\n",
    "\n",
    "    # Text reinigen\n",
    "    raw = clean_text(ch.text or \"\")\n",
    "    if len(raw) < 30:\n",
    "        continue\n",
    "\n",
    "    # Kontextualisierter Text\n",
    "    ctx = clean_text(chunker.contextualize(chunk=ch))\n",
    "\n",
    "    # Token zählen\n",
    "    txt_tokens = tokenizer.count_tokens(raw)\n",
    "    ctx_tokens = tokenizer.count_tokens(ctx)\n",
    "\n",
    "    print(f\"Raw ({txt_tokens} tokens): {raw[:500]!r}\\n\")\n",
    "    print(f\"Context ({ctx_tokens} tokens): {ctx[:500]!r}\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada37e2b-0447-4b59-8a77-91523d4e6044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Einfache Bereinigung (wie zuvor besprochen).\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    import re\n",
    "    s = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", s)     # Silbentrennung\n",
    "    s = s.replace(\"\\r\", \"\").strip()\n",
    "    s = re.sub(r\"\\n+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# Ausgabe-Ordner bestimmen\n",
    "source_path= Path(\"/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents/Nano Family/Nano 33 BLE/Nano_33_BLE_datasheet.pdf\")\n",
    "out_dir = Path(\"/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/Nano Family/Nano 33 BLE\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"chunks.jsonl\"\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, ch in enumerate(chunks):\n",
    "        raw = clean_text(ch.text or \"\")\n",
    "        if len(raw) < 30:  # zu kurze ignorieren\n",
    "            continue\n",
    "        ctx = clean_text(chunker.contextualize(chunk=ch))\n",
    "\n",
    "        meta = {\n",
    "            \"source\": str(source_path),\n",
    "            \"chunk_index\": i,\n",
    "            \"page_span\": getattr(ch, \"page_span\", None),\n",
    "        }\n",
    "\n",
    "        row = {\n",
    "            \"id\": f\"{Path(source_path).stem}::c{i}\",\n",
    "            \"text_raw\": raw,\n",
    "            \"text_ctx\": ctx,\n",
    "            \"meta\": meta,\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"[OK] {out_path} geschrieben ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb3bc61-7adb-43e3-a06e-d77ea6a16e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json (\"file:/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/Nano Family/Nano 33 BLE/chunks.jsonl\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39506c54-d817-4774-9146-66a4c52bdc2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"=== {i} ===\")\n",
    "    txt_tokens = tokenizer.count_tokens(chunk.text)\n",
    "    print(f\"chunk.text ({txt_tokens} tokens):\\n{chunk.text!r}\")\n",
    "\n",
    "    ser_txt = chunker.contextualize(chunk=chunk)\n",
    "    ser_tokens = tokenizer.count_tokens(ser_txt)\n",
    "    print(f\"chunker.contextualize(chunk) ({ser_tokens} tokens):\\n{ser_txt!r}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb7abd3-df2e-4fbf-899c-5bb9c4daed8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks-Notebook Zelle\n",
    "!pip install readability-lxml lxml beautifulsoup4 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee006fb-0cf9-4729-a4ff-f41805d4f0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def resolve_main_root() -> Path:\n",
    "    \"\"\"\n",
    "    Sucht im aktuellen Arbeitsverzeichnis (und Eltern) nach einem Ordner 'main'\n",
    "    mit einem Unterordner 'documents'. Funktioniert robust im Notebook.\n",
    "    \"\"\"\n",
    "    cwd = Path.cwd()\n",
    "    for base in [cwd, *cwd.parents]:\n",
    "        m = base / \"main\"\n",
    "        if (m / \"documents\").exists():\n",
    "            return m\n",
    "    raise RuntimeError(\"Konnte 'main/documents' nicht finden – bitte Notebook im Repo/Workspace öffnen.\")\n",
    "\n",
    "main_root = resolve_main_root()\n",
    "doc_root = main_root / \"documents\"\n",
    "doc_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8168e5c-8ff4-46d9-b045-0d7c3da9239e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Beispiel: wähle exakt deine beiden Dateien\n",
    "#/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents/Nano Family/Nano 33 BLE/Controlling RGB LED Through Bluetooth® _ Arduino Documentation.html\n",
    "# /Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents/Nano Family/Nano 33 BLE/Connecting Two Nano 33 BLE Boards Through I2C _ Arduino Documentation.html\n",
    "html_paths = [\n",
    "    doc_root / \"Nano Family\" / \"Nano 33 BLE\" / \"Controlling RGB LED Through Bluetooth® _ Arduino Documentation.html\",\n",
    "    doc_root / \"Nano Family\" / \"Nano 33 BLE\" / \"Connecting Two Nano 33 BLE Boards Through I2C _ Arduino Documentation.html\",\n",
    "]\n",
    "\n",
    "# Optional: falls du einfach die ersten 2-3 Tutorials automatisch nehmen willst:\n",
    "# html_paths = list(doc_root.rglob(\"*/tutorials/*.html\"))[:3]\n",
    "\n",
    "# Existenz prüfen:\n",
    "for p in html_paths:\n",
    "    print(p, \"EXISTS\" if p.exists() else \"MISSING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025a2e39-1415-461f-bb55-1191db016610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_with_readability(path: Path):\n",
    "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    doc = Document(raw)\n",
    "    title = doc.short_title() or path.stem\n",
    "    content_html = doc.summary()  # gereinigtes HTML\n",
    "    # Zusätzlich Plaintext extrahieren (optional, praktisch zum Debuggen)\n",
    "    text = BeautifulSoup(content_html, \"html.parser\").get_text(\"\\n\", strip=True)\n",
    "    return {\n",
    "        \"path\": str(path),\n",
    "        \"title\": title,\n",
    "        \"clean_html\": content_html,\n",
    "        \"clean_text\": text\n",
    "    }\n",
    "\n",
    "cleaned = [clean_html_with_readability(p) for p in html_paths if p.exists()]\n",
    "len(cleaned), [c[\"title\"] for c in cleaned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ebb5dc-072d-4c54-9689-0088f9739870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HTML-Vorschau (nacheinander)\n",
    "for item in cleaned:\n",
    "    print(\"=== TITLE:\", item[\"title\"])\n",
    "    displayHTML(item[\"clean_html\"])  # Databricks rendert den bereinigten Inhalt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d205620-b0fb-46cd-8dcf-8cdb62cdb31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Erste 800 Zeichen Plaintext je Datei ausgeben\n",
    "for item in cleaned:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TITLE:\", item[\"title\"])\n",
    "    print(\"PATH :\", item[\"path\"])\n",
    "    print(\"-\"*80)\n",
    "    preview = item[\"clean_text\"][:800]\n",
    "    print(preview + (\"...\" if len(item[\"clean_text\"]) > 800 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291293e3-d1a3-4764-8ba2-f926098546e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install trafilatura --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c738564c-ddb5-4aa1-9905-d3ccec2a7c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_html_with_trafilatura(path: Path):\n",
    "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    downloaded = trafilatura.extract(raw, include_comments=False, include_tables=True, include_links=False)\n",
    "    return {\n",
    "        \"path\": str(path),\n",
    "        \"title\": path.stem,\n",
    "        \"clean_text\": downloaded or \"\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd02d9a-9908-4515-b6f4-7c9a3b6f46b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_traf = [clean_html_with_trafilatura(p) for p in html_paths if p.exists()]\n",
    "print(len(cleaned_traf), \"files processed\")\n",
    "\n",
    "for c in cleaned_traf:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FILE:\", c[\"path\"])\n",
    "    print(\"EXTRACTED TEXT PREVIEW:\\n\")\n",
    "    print(c[\"clean_text\"][:800] + \"...\" if len(c[\"clean_text\"]) > 800 else c[\"clean_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2047b630-7e74-4965-a16d-478e4fbf1e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([{\"file\": c[\"path\"], \"chars\": len(c[\"clean_text\"])} for c in cleaned_traf])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f252ddf6-96ae-4c0c-9cf1-79ffdcfe30fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def peek(path: Path, n=1200):\n",
    "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    print(raw[:n])\n",
    "p1 = Path(\"/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents/Nano Family/Nano 33 BLE/Controlling RGB LED Through Bluetooth® _ Arduino Documentation.html\")\n",
    "peek(p1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fced157-bbe0-4e7a-9ec7-c506e9a18bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Databricks (oder lokal):\n",
    "# Falls schon installiert, kannst du die Zeile überspringen.\n",
    "%pip install --quiet langchain-text-splitters langchain-core beautifulsoup4 pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6551b1b0-87be-4290-a924-d24ed0e8ed95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INPUT_ROOT = Path(\"/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/documents\")  # Wurzel mit Kategorien/Produkten\n",
    "OUTPUT_ROOT = Path(\"/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out\")       # Hierhin schreiben wir JSONL\n",
    "\n",
    "# Teste zuerst EIN Produkt (step-by-step).\n",
    "TEST_CATEGORY = \"Nano Family\"   # z.B. \"Nano\"\n",
    "TEST_PRODUCT  = \"Nano 33 BLE\"    # z.B. \"A000005\"\n",
    "\n",
    "# Chunk-Parameter (gute Startwerte für technische Texte)\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "INPUT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d105b797-63cd-40a0-a015-2112875541cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_text_post(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Post-Cleaning:\n",
    "    - Markdown-Tabellen-Separatoren und Pipe-Blöcke weglassen (heuristisch),\n",
    "    - Linien-/Deko-Zeilen entfernen,\n",
    "    - Mehrfach-Leerzeilen reduzieren,\n",
    "    - Trim.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    skip_table_block = False\n",
    "    for line in text.splitlines():\n",
    "        l = line.strip()\n",
    "        # Trennlinie/Separator einer Markdown-Tabelle?\n",
    "        if re.match(r\"^\\|?(\\s*:?-{3,}:?\\s*\\|)+\\s*$\", l):\n",
    "            skip_table_block = True\n",
    "            continue\n",
    "        if skip_table_block:\n",
    "            if not l or \"|\" not in l:\n",
    "                skip_table_block = False\n",
    "            continue\n",
    "        # Deko-Linien (-----, =====, _____)\n",
    "        if re.match(r\"^[\\-_=\\s]{5,}$\", l):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "    text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "def read_pdf_clean(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extrahiert reinen Text aus PDF (ohne Bilder).\n",
    "    Hinweis: Tabellen in PDFs sind oft als Text 'gezeichnet' → heuristische Nachreinigung.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                parts.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] PDF konnte nicht gelesen werden: {pdf_path} -> {e}\")\n",
    "        return \"\"\n",
    "    return clean_text_post(\"\\n\".join(parts))\n",
    "    \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "SECTION_PATTERNS = [\n",
    "    r'^#{1,6}\\s+(.+)$',        # Markdown-Header\n",
    "    r'^.+\\n[=\\-]{2,}\\s*$',     # unterstrichene Header\n",
    "    r'^[A-Z][A-Z\\s0-9\\-_/]+:$' # ALL CAPS mit Doppelpunkt\n",
    "]\n",
    "STOPWORDS = set(['the','and','is','of','to','a','in','that','it','with','as','for','on','this','by','an','be'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011a1275-93b4-41bd-98e2-537d30460ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def perform_semantic_chunking(document_text: str, chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_text(document_text)\n",
    "\n",
    "    docs: List[Document] = []\n",
    "    current_section = \"Introduction\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        for line in chunk.splitlines():\n",
    "            line = line.strip()\n",
    "            for pat in SECTION_PATTERNS:\n",
    "                m = re.match(pat, line, flags=re.MULTILINE)\n",
    "                if m:\n",
    "                    current_section = m.group(1) if m.groups() else m.group(0)\n",
    "                    break\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', chunk.lower())\n",
    "        density = 0.0 if not words else round(\n",
    "            len([w for w in words if w not in STOPWORDS]) / max(1, len(words)), 3\n",
    "        )\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"chunk_id\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"chunk_size\": len(chunk),\n",
    "                    \"chunk_type\": \"semantic\",\n",
    "                    \"section\": current_section,\n",
    "                    \"semantic_density\": density\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4817f64a-2bcc-45df-8838-679fd966e86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_product_texts(category: str, product: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Liest alle relevanten Dateien eines Produkts und gibt Liste (source_file_name, cleaned_text) zurück.\n",
    "    Erwartete Struktur:\n",
    "      main/documents/<category>/<product>/{*.pdf, tutorial_*.html}\n",
    "    \"\"\"\n",
    "    base = INPUT_ROOT / category / product\n",
    "    results: List[Tuple[str, str]] = []\n",
    "\n",
    "    # PDFs\n",
    "    for pdf in sorted(base.glob(\"*.pdf\")):\n",
    "        t = read_pdf_clean(pdf)\n",
    "        if t.strip():\n",
    "            results.append((pdf.name, t))\n",
    "\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "read_product_texts(category=TEST_CATEGORY, product=TEST_PRODUCT) \n",
    "\n",
    "read_product_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7749f2cb-b8ff-499a-8e30-db717431489b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_master_text(sources: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Kombiniert mehrere Quelldateien in einen Text mit klaren SOURCE-Markern.\n",
    "    Der Splitter findet dadurch natürliche Grenzen besser.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for name, txt in sources:\n",
    "        parts.append(f\"## SOURCE: {name}\\n\\n{txt.strip()}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n",
    "make_master_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869c2497-5531-4b0d-9f68-591008ddfd49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_jsonl(records, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def process_one_product(category: str, product: str, chunk_size: int, chunk_overlap: int) -> int:\n",
    "    sources = read_product_texts(category, product)\n",
    "    if not sources:\n",
    "        print(f\"[INFO] Keine Quellen unter {INPUT_ROOT / category / product}\")\n",
    "        return 0\n",
    "\n",
    "    master_text = make_master_text(sources)\n",
    "    docs = perform_semantic_chunking(master_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    # JSONL-Records\n",
    "    records = []\n",
    "    for d in docs:\n",
    "        records.append({\n",
    "            \"category\": category,\n",
    "            \"product\": product,\n",
    "            \"chunk_id\": f\"{category}:{product}:{d.metadata['chunk_id']:04d}\",\n",
    "            \"total_chunks\": d.metadata[\"total_chunks\"],\n",
    "            \"chunk_type\": d.metadata[\"chunk_type\"],\n",
    "            \"section\": d.metadata[\"section\"],\n",
    "            \"semantic_density\": d.metadata[\"semantic_density\"],\n",
    "            \"chunk_size\": d.metadata[\"chunk_size\"],\n",
    "            \"text\": d.page_content\n",
    "        })\n",
    "\n",
    "    out_path = OUTPUT_ROOT / category / product / \"chunks_sample.jsonl\"\n",
    "    write_jsonl(records, out_path)\n",
    "    print(f\"[OK] {category}/{product}: {len(records)} Chunks → {out_path}\")\n",
    "    return len(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a63a59-61e5-404a-9621-cd1a9cb1f0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = process_one_product(\n",
    "    TEST_CATEGORY,\n",
    "    TEST_PRODUCT,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8f2d41-b711-4b38-a741-f9ec2621a5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "\n",
    "out_file = OUTPUT_ROOT / TEST_CATEGORY / TEST_PRODUCT / \"chunks_sample.jsonl\"\n",
    "\n",
    "df = spark.read.json(\"file:/Workspace/Users/nasiba.tuychieva@gea.com/ai_samples/main/out/Nano Family/Nano 33 BLE/chunks_sample.jsonl\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24e8041-7b86-4fa4-a48c-f55f7ca0e0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ➜ Passe diese beiden Pfade an DEIN Projekt an:\n",
    "INPUT_ROOT = Path(\"main/documents\")  # Wurzel mit Kategorien/Produkten\n",
    "OUTPUT_ROOT = Path(\"main/out\")       # Hierhin schreiben wir JSONL\n",
    "\n",
    "# Teste zuerst EIN Produkt (step-by-step).\n",
    "TEST_CATEGORY = \"your_category\"   # z.B. \"Nano\"\n",
    "TEST_PRODUCT  = \"your_product\"    # z.B. \"A000005\"\n",
    "\n",
    "# Chunk-Parameter (gute Startwerte für technische Texte)\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b330daf0-b060-41dc-b1f4-cc052c799f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "def perform_semantic_chunking(document, chunk_size=500, chunk_overlap=100:disappointed_face:\n",
    "    \"\"\"\n",
    "    Performs semantic chunking on a document using recursive character splitting \n",
    "    at logical text boundaries.\n",
    "    \n",
    "    Args:\n",
    "        document (str): The text document to process\n",
    "        chunk_size (int): The target size of each chunk in characters\n",
    "        chunk_overlap (int): The number of characters of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: The semantically chunked documents with metadata\n",
    "    \"\"\"\n",
    "    # Create the text splitter with semantic separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    # Split the text into semantic chunks\n",
    "    semantic_chunks = text_splitter.split_text(document)\n",
    "    print(f\"Document split into {len(semantic_chunks)} semantic chunks\")\n",
    "    \n",
    "    # Determine section titles for enhanced metadata\n",
    "    section_patterns = [\n",
    "        r'^#+\\s+(.+)$',      # Markdown headers\n",
    "        r'^.+\\n[=\\-]{2,}$',  # Underlined headers\n",
    "        r'^[A-Z\\s]+:$'       # ALL CAPS section titles\n",
    "    ]\n",
    "    \n",
    "    # Convert to Document objects with enhanced metadata\n",
    "    documents = []\n",
    "    current_section = \"Introduction\"\n",
    "    \n",
    "    for i, chunk in enumerate(semantic_chunks):\n",
    "        # Try to identify section title from chunk\n",
    "        chunk_lines = chunk.split('\\n')\n",
    "        for line in chunk_lines:\n",
    "            for pattern in section_patterns:\n",
    "                match = re.match(pattern, line, re.MULTILINE)\n",
    "                if match:\n",
    "                    current_section = match.group(0)\n",
    "                    break\n",
    "        \n",
    "        # Calculate semantic density (ratio of non-stopwords to total words)\n",
    "        words = re.findall(r'\\b\\w+\\b', chunk.lower())\n",
    "        stopwords = ['the', 'and', 'is', 'of', 'to', 'a', 'in', 'that', 'it', 'with', 'as', 'for']\n",
    "        content_words = [w for w in words if w not in stopwords]\n",
    "        semantic_density = len(content_words) / max(1, len(words))\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"total_chunks\": len(semantic_chunks),\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"chunk_type\": \"semantic\",\n",
    "                \"section\": current_section,\n",
    "                \"semantic_density\": round(semantic_density, 2)\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "langchain_code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
