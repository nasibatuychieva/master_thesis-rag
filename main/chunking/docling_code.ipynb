{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2095bcb7-7e6c-4ea1-a7b6-61f2ec62df9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install docling\n",
    "!pip install -qU pip docling transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf0ef3ac-aa06-4732-ae8d-dd7445d06678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.dbutils import DBUtils\n",
    "import re, os\n",
    "\n",
    "def get_repo_root() -> Path:\n",
    "    dbutils = DBUtils(spark)\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "    # Extrahiere /Workspace/Repos/<user>/<repo>\n",
    "    match = re.match(r\"^(\\/Workspace\\/Repos\\/[^\\/]+\\/[^\\/]+)\", notebook_path)\n",
    "    if match:\n",
    "        return Path(match.group(1))\n",
    "    else:\n",
    "        return Path(os.getcwd())\n",
    "\n",
    "repo_root = get_repo_root()\n",
    "print(\"📁 Repo root:\", repo_root)\n",
    "\n",
    "# Eine Ebene höher (wie \"cd ..\")\n",
    "parent_path = repo_root.parent\n",
    "\n",
    "# Zwei Ebenen höher\n",
    "two_up = repo_root.parent.parent\n",
    "\n",
    "print(\"⬆️ Eine Ebene höher:\", parent_path)\n",
    "print(\"⬆️ root:\", parent_path)\n",
    "\n",
    "doc_root = parent_path / \"documents\"\n",
    "out_dir = parent_path / \"out\"\n",
    "product_name = \"Nano 33 BLE\"\n",
    "product_category = \"Nano Family\"\n",
    "file_name  = \"Nano_33_BLE_datasheet.pdf\"\n",
    "\n",
    "pdf_path = doc_root / product_category / product_name / \"Nano_33_BLE_datasheet.pdf\"\n",
    "print(\"⬆️ pdf pfad:\",pdf_path)\n",
    "\n",
    "out_path_docling = out_dir / product_category / product_name / \"docling_chunks.jsonl\"\n",
    "print(\"⬆️ oout_path_docling:\",out_path_docling)\n",
    "\n",
    "out_path_langchain = out_dir / product_category / product_name / \"langchain_chunks.jsonl\"\n",
    "print(\"⬆️ out_path_langchain:\",out_path_langchain)\n",
    "\n",
    "code_path_langchain = parent_path / \"langchain_chunking.py\"\n",
    "print(\"⬆️code_path_langchain:\",code_path_langchain)\n",
    "\n",
    "code_path_docling= parent_path / \"docling_chunking.py\"\n",
    "print(\"⬆️code_path_langchain:\",code_path_docling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4080490-68e9-4231-9d1d-32d077a35fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "pdf_options = PdfPipelineOptions()\n",
    "pdf_options.do_ocr = False                    # No OCR - pure text extraction only   # noqa: E501\n",
    "pdf_options.generate_page_images = False      # No page images  # noqa: E501\n",
    "pdf_options.generate_picture_images = False   # Ignore pictures completely  # noqa: E501\n",
    "pdf_options.generate_table_images = False     # Keep tables as text/markdown, not images  # noqa: E501\n",
    "\n",
    "        # Configure format options\n",
    "format_options = {\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pdf_options\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Initialize document converter\n",
    "converter_2 = DocumentConverter(\n",
    "            format_options=format_options\n",
    "        )\n",
    "result_2 = converter_2.convert(pdf_path)\n",
    "doc_2 = result_2.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d985edd-e2de-4c10-8af7-41c538254164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(doc_2.pages)\n",
    "print(doc_2.export_to_markdown()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c39cde7-ac4a-4e4c-83a8-f1cce4a50b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 800  # set to a small number for illustrative purposes\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "    max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    ")\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    merge_peers=True,  # optional, defaults to True\n",
    ")\n",
    "chunk_iter = chunker.chunk(dl_doc=doc_2)\n",
    "chunks = list(chunk_iter)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe3dec2-eadc-4f46-8bde-ad8e0c845e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning-Funktion\n",
    "# ---------------------------\n",
    "def clean_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", t)\n",
    "    t = t.replace(\"\\r\", \"\")\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\", t)\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t)\n",
    "\n",
    "    def noisy(line: str) -> bool:\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            return True\n",
    "        non_alpha = sum(1 for ch in s if not ch.isalpha())\n",
    "        return (non_alpha / max(1, len(s))) > 0.6\n",
    "\n",
    "    lines = [ln for ln in t.split(\"\\n\") if not noisy(ln)]\n",
    "    t = \"\\n\".join(lines)\n",
    "    t = re.sub(r\"^(Table|Figure)\\s*\\d+[:.\\-]\\s.*$\", \"\", t, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    t = re.sub(r\"^\\s*\\|.*\\|\\s*$\", \"\", t, flags=re.MULTILINE)\n",
    "    t = re.sub(r\"^\\s*[-=]{3,}\\s*$\", \"\", t, flags=re.MULTILINE)\n",
    "    return t.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a300d9-bea0-429f-bc2e-135f03694db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "total_chunks = len(chunks)\n",
    "parts = pdf_path.parts\n",
    "category = parts[-3]\n",
    "product = parts[-2]\n",
    "records = []\n",
    "for i, ch in enumerate(chunks):\n",
    "    text_raw = clean_text(ch.text or \"\")\n",
    "    if len(text_raw) < 30:\n",
    "            continue\n",
    "\n",
    "    context = clean_text(chunker.contextualize(chunk=ch))\n",
    "\n",
    "        # Kategorie und Abschnitt bestimmen (heuristisch)\n",
    "    section = None\n",
    "    try:\n",
    "        hierarchy = getattr(ch, \"hierarchy_path\", None)\n",
    "        if hierarchy and isinstance(hierarchy, list) and len(hierarchy) > 0:\n",
    "            section = hierarchy[-1].get(\"title\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "        # Semantische Dichte = Tokens / Zeichen (gibt an, wie „kompakt“ der Text ist)\n",
    "    n_tokens = tokenizer.count_tokens(context)\n",
    "    semantic_density = round(n_tokens / max(1, len(context)), 4)\n",
    "\n",
    "    rec = {\n",
    "            \"category\": category,\n",
    "            \"chunk_id\": f\"{Path(pdf_path).stem}::c{i}\",\n",
    "            \"chunk_size\": n_tokens,\n",
    "            \"chunk_type\": \"contextualized\",\n",
    "            \"product\": product,\n",
    "            \"section\": section,\n",
    "            \"semantic_density\": semantic_density,\n",
    "            \"text\": context,\n",
    "            \"total_chunks\": total_chunks\n",
    "        }\n",
    "    records.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0382c9a5-683b-4ab4-b5f6-657d74ad66e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_path_docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26779ec6-4418-4294-b847-307d5b9a5bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# In JSONL speichern\n",
    "import json\n",
    "with open (out_path_docling, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"[OK] {len(records)} Chunks gespeichert unter: {out_path_docling}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb3bc61-7adb-43e3-a06e-d77ea6a16e2a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761485528794}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_docling = spark.read.json (f\"file:{out_path_docling}\")\n",
    "df_docling.select(\"text\", \"chunk_id\", \"chunk_size\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "docling_code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
